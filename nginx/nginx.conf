user nginx;
worker_processes 4;

error_log  /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
  # determines how much clients will be served per worker
  # max clients = worker_connections * worker_processes
  # max clients is also limited by the number of socket connections available on the system (~64k)
  worker_connections 4000;

  # optmized to serve many clients with each thread, essential for linux -- for testing environment
  use epoll;

  # accept as many connections as possible, may flood worker connections if set too low -- for testing environment
  multi_accept on;
}

http {
  # Enables or disables emitting nginx version on error pages and in the “Server” response header field.
  server_tokens off;

  # If you’re serving locally stored static files,
  # sendfile is totally essential to speed your Web server.
  # But if you use Nginx as a reverse proxy to serve pages from
  # an application server, you can deactivate it. Unless you start
  # serving micro caching on a tmpfs. I’ve been doing it here, and didn’t
  # even notice the day I was featured on HN homepage, Reddit or good old Slashdot.
  sendfile on;
  # send headers in one peace, its better then sending them one by one
  tcp_nopush on;
  # don't buffer data sent, good for small data bursts in real time
  tcp_nodelay on;
  keepalive_timeout 15;
  types_hash_max_size 2048;
  include /etc/nginx/mime.types;
  default_type application/octet-stream;
  #access_log off;
  #error_log off;
  gzip on;
  gzip_disable "msie6";
  include /etc/nginx/conf.d/*.conf;
  include /etc/nginx/sites-enabled/*;
  open_file_cache max=100;
}